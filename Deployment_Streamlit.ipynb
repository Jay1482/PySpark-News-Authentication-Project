{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcI1FyosnSLO"
      },
      "outputs": [],
      "source": [
        "!pip install streamlit\n",
        "!pip install pyspark py4j"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzyqw8DpqZHl"
      },
      "source": [
        "For Writing cell python code into an app.py file"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit_lottie"
      ],
      "metadata": {
        "id": "xd1YPvq6ueP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GRpCEPRinWKu"
      },
      "outputs": [],
      "source": [
        "# %%writefile app.py\n",
        "# import streamlit as st\n",
        "# st.write('#Hello World')\n",
        "# st.write('## Run Streamlit on colab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48dMHwaHv4Jv"
      },
      "source": [
        "Here We are using Local tunnel (It exposes your localhost to the world which avoids the usage of ngrok) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEIqkKNhnwJO"
      },
      "outputs": [],
      "source": [
        "#!streamlit run app.py & npx localtunnel --port 8501"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cN1spl7B5ukI",
        "outputId": "010d0b5f-b72c-4640-de56-85a6f7394dd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MH83HYp04S-t"
      },
      "source": [
        "For running fapp.py file which is stored just aside drive and localtunnel exposes your localhost to the world which avoids the usage of ngrok.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DILYYtDA_CK",
        "outputId": "06101dca-9874-4de8-d7be-99640f866b31"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K\u001b[?25hnpx: installed 22 in 2.883s\n",
            "your url is: https://icy-results-take-34-148-235-0.loca.lt\n",
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to False.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.148.235.0:8501\u001b[0m\n",
            "\u001b[0m\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "23/03/15 14:15:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/usr/local/lib/python3.9/dist-packages/pyspark/jars/spark-core_2.12-3.3.2.jar) to field java.math.BigInteger.mag\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "2023-03-15 14:16:14.495 Closing down clientserver connection\n",
            "2023-03-15 14:17:50.923 Closing down clientserver connection\n",
            "2023-03-15 14:18:25.614 Closing down clientserver connection\n",
            "2023-03-15 14:18:35.130 Closing down clientserver connection\n",
            "/usr/local/lib/python3.9/dist-packages/pyspark/sql/dataframe.py:3313: FutureWarning: DataFrame.to_pandas_on_spark is deprecated. Use DataFrame.pandas_api instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/__init__.py:49: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
            "  warnings.warn(\n",
            "23/03/15 14:18:43 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
            "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_numpy` loads all data into the driver's memory. It should only be used if the resulting NumPy ndarray is expected to be small.\n",
            "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
            "2023-03-15 14:18:51.581 Closing down clientserver connection\n",
            "23/03/15 14:19:44 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
            "2023-03-15 14:19:55.590 Closing down clientserver connection\n",
            "2023-03-15 14:20:36.669 Closing down clientserver connection\n",
            "2023-03-15 14:20:40.239 Closing down clientserver connection\n",
            "2023-03-15 14:20:43.942 Closing down clientserver connection\n",
            "/usr/local/lib/python3.9/dist-packages/pyspark/sql/dataframe.py:3313: FutureWarning: DataFrame.to_pandas_on_spark is deprecated. Use DataFrame.pandas_api instead.\n",
            "  warnings.warn(\n",
            "23/03/15 14:21:14 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
            "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_numpy` loads all data into the driver's memory. It should only be used if the resulting NumPy ndarray is expected to be small.\n",
            "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
            "2023-03-15 14:21:21.831 Closing down clientserver connection\n"
          ]
        }
      ],
      "source": [
        "!streamlit run main_deployment.py & npx localtunnel --port 8501"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HfiktxLPuqi-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}